{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Perceptron and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll try out the multiclass Perceptron and SVM on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiclass Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the code for the multiclass Perceptron algorithm. This is similar in spirit to our earlier binary Perceptron algorithm, except that now there is a linear function for each class.\n",
    "\n",
    "If there are `k` classes, we will assume that they are numbered `0,1,...,k-1`. For `d`-dimensional data, the classifier will be parametrized by:\n",
    "* `w`: this is a `kxd` numpy array with one row for each class\n",
    "* `b`: this is a `k`-dimensional numpy array with one offset for each class\n",
    "\n",
    "Thus the linear function for class `j` (where `j` lies in the range `0` to `k-1`) is given by `w[j,:], b[j]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first procedure, **evaluate_classifier**, takes as input the parameters of a linear classifier (`w,b`) as well as a data point (`x`) and returns the prediction of that classifier at `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(w,b,x):\n",
    "    k = len(b)\n",
    "    scores = np.zeros(k)\n",
    "    for j in range(k):\n",
    "        scores[j] = np.dot(w[j,:],x) + b[j]\n",
    "    return int(np.argmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the multiclass Perceptron training procedure. It is invoked as follows:\n",
    "* `w,b,converged = train_multiclass_perceptron(x,y,k,n_iters)`\n",
    "\n",
    "where\n",
    "* `x`: n-by-d numpy array with n data points, each d-dimensional\n",
    "* `y`: n-dimensional numpy array with the labels (in the range `0` to `k-1`)\n",
    "* `k`: the number of classes\n",
    "* `n_iters`: the training procedure will run through the data at most this many times (default: 100)\n",
    "* `w,b`: parameters for the final linear classifier, as above\n",
    "* `converged`: flag (True/False) indicating whether the algorithm converged within the prescribed number of iterations\n",
    "\n",
    "If the data is not linearly separable, then the training procedure will not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_multiclass_perceptron(x,y,k,n_iters=100):\n",
    "    n,d = x.shape\n",
    "    w = np.zeros((k,d))\n",
    "    b = np.zeros(k)\n",
    "    done = False\n",
    "    converged = True\n",
    "    iters = 0\n",
    "    np.random.seed(None)\n",
    "    while not(done):\n",
    "        done = True\n",
    "        I = np.random.permutation(n)\n",
    "        for j in I:\n",
    "            pred_y = evaluate_classifier(w,b,x[j,:])\n",
    "            true_y = int(y[j])\n",
    "            if pred_y != true_y:\n",
    "                w[true_y,:] = w[true_y,:] + x[j,:]\n",
    "                b[true_y] = b[true_y] + 1.0\n",
    "                w[pred_y,:] = w[pred_y,:] - x[j,:]\n",
    "                b[pred_y] = b[pred_y] - 1.0\n",
    "                done = False\n",
    "        iters = iters + 1\n",
    "        if iters > n_iters:\n",
    "            done = True\n",
    "            converged = False\n",
    "    if converged:\n",
    "        print \"Perceptron algorithm: iterations until convergence: \", iters\n",
    "    else:\n",
    "        print \"Perceptron algorithm: did not converge within the specified number of iterations\"\n",
    "    return w, b, converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments with multiclass Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next routine takes as input a two-dimensional data set as well as a classifier. It plots the points, with different colors for different labels, and shows the decision boundaries of the classifier. It is invoked as follows:\n",
    "* `display_data_and_boundary(x,y,pred_fn)`\n",
    "\n",
    "where\n",
    "* `x` and `y` are the two-dimensional data and their labels (in the range `0,...,k-1`)\n",
    "* `pred_fn` is the classifier: it is a function that takes a data point and returns a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_data_and_boundary(x,y,pred_fn):\n",
    "    # Determine the x1- and x2- limits of the plot\n",
    "    x1min = min(x[:,0]) - 1\n",
    "    x1max = max(x[:,0]) + 1\n",
    "    x2min = min(x[:,1]) - 1\n",
    "    x2max = max(x[:,1]) + 1\n",
    "    plt.xlim(x1min,x1max)\n",
    "    plt.ylim(x2min,x2max)\n",
    "    # Plot the data points\n",
    "    k = int(max(y)) + 1\n",
    "    cols = ['ro', 'k^', 'b*','gx']\n",
    "    for label in range(k):\n",
    "        plt.plot(x[(y==label),0], x[(y==label),1], cols[label%4], markersize=8)\n",
    "    # Construct a grid of points at which to evaluate the classifier\n",
    "    grid_spacing = 0.05\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1min, x1max, grid_spacing), np.arange(x2min, x2max, grid_spacing))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    Z = np.array([pred_fn(pt) for pt in grid])\n",
    "    # Show the classifier's boundary using a color plot\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.pcolormesh(xx1, xx2, Z, cmap=plt.cm.Pastel1, vmin=0, vmax=k)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following procedure, **run_multiclass_perceptron**, loads a labeled two-dimensional data set, learns a linear classifier using the Perceptron algorithm, and then displays the data as well as the boundary.\n",
    "\n",
    "The data file is assumed to contain one data point per line, along with a label, like:\n",
    "* `3 8 2` (meaning that point `x=(3,8)` has label `y=2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_multiclass_perceptron(datafile):\n",
    "    data = np.loadtxt(datafile)\n",
    "    n,d = data.shape\n",
    "    # Create training set x and labels y\n",
    "    x = data[:,0:2]\n",
    "    y = data[:,2]\n",
    "    k = int(max(y)) + 1\n",
    "    print \"Number of classes: \", k\n",
    "    # Run the Perceptron algorithm for at most 1000 iterations\n",
    "    w,b,converged = train_multiclass_perceptron(x,y,k,1000)\n",
    "    # Show the data and boundary\n",
    "    pred_fn = lambda p: evaluate_classifier(w,b,p)\n",
    "    display_data_and_boundary(x,y,pred_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out on two simple data sets. Make sure that the directory containing this notebook also contains the two-dimensional data files `data_3.txt` and `data_4.txt`. You should run these next two cells a few times to get a sense of the variability of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_perceptron('data_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_perceptron('data_4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments with multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how multiclass SVM fares on these same data sets. We start with an analog of the **run_multiclass_perceptron** function. The key difference is that the SVM version, **run_multiclass_svm**, takes a second parameter: the regularization constant `C` in the convex program of the soft-margin SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "def run_multiclass_svm(datafile,C_value=1.0):\n",
    "    data = np.loadtxt(datafile)\n",
    "    n,d = data.shape\n",
    "    # Create training set x and labels y\n",
    "    x = data[:,0:2]\n",
    "    y = data[:,2]\n",
    "    k = int(max(y)) + 1\n",
    "    print \"Number of classes: \", k\n",
    "    # Train an SVM\n",
    "    clf = LinearSVC(loss='hinge', multi_class='crammer_singer', C=C_value)\n",
    "    clf.fit(x,y)\n",
    "    # Show the data and boundary\n",
    "    pred_fn = lambda p: clf.predict(p.reshape(1,-1))    \n",
    "    display_data_and_boundary(x,y,pred_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this on the two data sets `data_3.txt` and `data_4.txt` that we saw earlier. Try playing with the second parameter to see how the decision boundary changes. You should try values like `C = 0.01, 0.1, 1.0, 10.0, 100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_svm('data_3.txt',10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_svm('data_4.txt',100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">For you to think about:</font> How would you summarize the effect of varying `C`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final experiment is with the famous IRIS data set. This is four-dimensional data with three labels, but we will pick just two of the features, as a consequence of which the problem is not linearly separable. Thus the Perceptron algorithm would never converge. The soft-margin SVM obtains a reasonable solution, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS data\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "# Select just two of the four features\n",
    "features = [1,3]\n",
    "x = x[:,features]\n",
    "# Train SVM\n",
    "clf = LinearSVC(loss='hinge', multi_class='crammer_singer')\n",
    "clf.fit(x,y)\n",
    "pred_fn = lambda p: clf.predict(p.reshape(1,-1))\n",
    "display_data_and_boundary(x,y,pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
